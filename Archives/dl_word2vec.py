# -*- coding: utf-8 -*-
"""DL_Word2Vec.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18Q7o-1Ga86ITwqJeNmgp3d2bsanTEF99
"""

# ==============================================================================
# CELLULE 1 : CONFIGURATION ET INSTALLATION
# ==============================================================================

print("üöÄ CELLULE 1: Configuration initiale")
print("="*50)

# Installation des packages
!pip install -q nltk tensorflow tqdm

# Imports
import pandas as pd
import numpy as np
import tensorflow as tf
import re
import unicodedata
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from tqdm import tqdm
import matplotlib.pyplot as plt
import time
import pickle
import os

print("‚úÖ Packages install√©s et import√©s")

# Configuration GPU
print("\nüîß Configuration GPU...")
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        tf.config.experimental.set_memory_growth(gpus[0], True)
        print(f"‚úÖ GPU configur√©: {gpus[0].name}")
    except RuntimeError as e:
        print(f"‚ùå Erreur GPU: {e}")
else:
    print("‚ö†Ô∏è  Utilisation CPU")

# Montage Google Drive
print("\nüíæ Montage Google Drive...")
from google.colab import drive
drive.mount('/content/drive')

# Configuration des chemins
BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/DL_Word2Vec'
DATA_PATH = f'{BASE_PATH}/Data'
MODEL_PATH = f'{BASE_PATH}/Model'

# Cr√©ation des dossiers
for path in [BASE_PATH, DATA_PATH, MODEL_PATH]:
    os.makedirs(path, exist_ok=True)

print(f"‚úÖ Structure cr√©√©e:")
print(f"   üìÅ Base: {BASE_PATH}")
print(f"   üìä Data: {DATA_PATH}")
print(f"   üß† Model: {MODEL_PATH}")

# T√©l√©chargement NLTK
print("\nüìö Configuration NLTK...")
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)
stop_words = set(stopwords.words('english'))

print("‚úÖ CELLULE 1 TERMIN√âE - Passez √† la cellule 2")

# ==============================================================================
# CELLULE 2 : CHARGEMENT DES DONN√âES
# ==============================================================================

print("üöÄ CELLULE 2: Chargement des donn√©es")
print("="*50)

# Fonction de chargement robuste
def load_dataset():
    """Essaie de charger le dataset depuis diff√©rents emplacements"""
    locations = [
        f"{DATA_PATH}/MovieReview.csv",
        f"{BASE_PATH}/MovieReview.csv",
        "/content/MovieReview.csv",
        "MovieReview.csv"
    ]

    for location in locations:
        try:
            df = pd.read_csv(location)
            print(f"‚úÖ Dataset charg√© depuis: {location}")
            return df
        except FileNotFoundError:
            print(f"‚ùå Non trouv√©: {location}")
            continue

    print("\nüõë DATASET NON TROUV√â!")
    print("Veuillez uploader MovieReview.csv dans Colab ou le placer dans:")
    for loc in locations:
        print(f"   - {loc}")
    return None

# Chargement des donn√©es
df = load_dataset()

if df is not None:
    print(f"\nüìä Dataset charg√© avec succ√®s!")
    print(f"   üìè Shape: {df.shape}")
    print(f"   üìù Colonnes: {list(df.columns)}")

    # Aper√ßu des donn√©es
    print(f"\nüëÄ Aper√ßu des 3 premi√®res reviews:")
    for i in range(min(3, len(df))):
        review_text = str(df.review.iloc[i])[:100] + "..."
        print(f"   {i+1}. {review_text}")

    # Suppression de la colonne sentiment si elle existe
    if 'sentiment' in df.columns:
        df = df.drop('sentiment', axis=1)
        print("‚úÖ Colonne 'sentiment' supprim√©e")

    # Statistiques de base
    print(f"\nüìä Statistiques:")
    print(f"   - Reviews totales: {len(df):,}")
    print(f"   - Reviews non vides: {df.review.notna().sum():,}")
    print(f"   - Longueur moyenne: {df.review.str.len().mean():.0f} caract√®res")
    print(f"   - Longueur min/max: {df.review.str.len().min()} / {df.review.str.len().max()}")

    print("‚úÖ CELLULE 2 TERMIN√âE - Passez √† la cellule 3")
else:
    print("üõë ARR√äT - Veuillez d'abord charger le dataset")

# ==============================================================================
# CELLULE 3 : PREPROCESSING DES TEXTES
# ==============================================================================

print("üöÄ CELLULE 3: Preprocessing des textes")
print("="*50)

def preprocess_text(text):
    """Fonction de preprocessing compl√®te"""
    if pd.isna(text) or text == '':
        return ''

    # Conversion en string et minuscules
    text = str(text).lower().strip()

    # Suppression des accents
    text = unicodedata.normalize('NFD', text)
    text = ''.join(c for c in text if unicodedata.category(c) != 'Mn')

    # Espacement autour de la ponctuation
    text = re.sub(r"([?.!,¬ø])", r" \1 ", text)

    # Suppression des espaces multiples
    text = re.sub(r'[" "]+', " ", text)

    # Garde seulement lettres et ponctuation de base
    text = re.sub(r"[^a-zA-Z?.! ]+", " ", text)

    # Suppression des mots de 1-2 lettres
    text = re.sub(r'\b\w{1,2}\b', '', text)

    # Tokenisation et suppression des stop words
    try:
        tokens = word_tokenize(text.strip())
        tokens = [token for token in tokens if token not in stop_words and len(token) > 2]
        return ' '.join(tokens).strip()
    except:
        return text.strip()

# Test de la fonction sur un √©chantillon
print("üß™ Test de preprocessing:")
if 'df' in locals() and len(df) > 0:
    sample_text = df.review.iloc[0]
    processed = preprocess_text(sample_text)

    print(f"Original  : {str(sample_text)[:150]}...")
    print(f"Processed : {processed[:150]}...")

    # Application sur toutes les donn√©es
    print(f"\nüîÑ Application sur {len(df):,} reviews...")
    tqdm.pandas(desc="Preprocessing")
    df['review_clean'] = df.review.progress_apply(preprocess_text)

    # Nettoyage - garde seulement les reviews avec plus de 10 caract√®res
    df_clean = df[df.review_clean.str.len() > 10].copy()

    print(f"\nüìä R√©sultats du preprocessing:")
    print(f"   - Reviews originales: {len(df):,}")
    print(f"   - Reviews conserv√©es: {len(df_clean):,}")
    print(f"   - Taux de conservation: {len(df_clean)/len(df)*100:.1f}%")

    # Statistiques apr√®s preprocessing
    print(f"   - Longueur moyenne apr√®s: {df_clean.review_clean.str.len().mean():.0f} caract√®res")

    # Aper√ßu apr√®s preprocessing
    print(f"\nüëÄ Exemples apr√®s preprocessing:")
    for i in range(min(3, len(df_clean))):
        clean_text = df_clean.review_clean.iloc[i][:100] + "..."
        print(f"   {i+1}. {clean_text}")

    # Sauvegarde
    processed_file = f"{DATA_PATH}/preprocessed_reviews.pkl"
    df_clean.to_pickle(processed_file)
    print(f"\nüíæ Donn√©es sauvegard√©es: {processed_file}")

    print("‚úÖ CELLULE 3 TERMIN√âE - Passez √† la cellule 4")
else:
    print("üõë ERREUR - Variable 'df' non trouv√©e. Ex√©cutez d'abord la cellule 2")

# ==============================================================================
# CELLULE 4 : TOKENISATION
# ==============================================================================

print("üöÄ CELLULE 4: Tokenisation")
print("="*50)

if 'df_clean' not in locals():
    print("üõë ERREUR - Variable 'df_clean' non trouv√©e. Ex√©cutez d'abord la cellule 3")
else:
    # Configuration du tokenizer
    MAX_WORDS = 10000
    print(f"üìù Configuration tokenizer:")
    print(f"   - Vocabulaire maximum: {MAX_WORDS:,} mots")
    print(f"   - Token OOV: <OOV>")

    # Cr√©ation et entra√Ænement du tokenizer
    tokenizer = tf.keras.preprocessing.text.Tokenizer(
        num_words=MAX_WORDS,
        oov_token="<OOV>",
        filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n'
    )

    print(f"\nüîÑ Entra√Ænement sur {len(df_clean):,} reviews...")
    tokenizer.fit_on_texts(df_clean.review_clean)

    # Statistiques du vocabulaire
    vocab_size = min(len(tokenizer.word_index) + 1, MAX_WORDS)
    print(f"‚úÖ Tokenizer cr√©√©!")
    print(f"   - Mots uniques trouv√©s: {len(tokenizer.word_index):,}")
    print(f"   - Vocabulaire final: {vocab_size:,} mots")

    # Top 20 des mots les plus fr√©quents
    print(f"\nüìä Top 20 des mots les plus fr√©quents:")
    word_freq = tokenizer.word_counts
    top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:20]

    for i, (word, freq) in enumerate(top_words, 1):
        print(f"   {i:2d}. {word:<15} : {freq:,}")

    # Test de conversion
    print(f"\nüß™ Test de conversion:")
    test_sentence = df_clean.review_clean.iloc[0]
    test_sequence = tokenizer.texts_to_sequences([test_sentence])[0]

    print(f"Texte original: {test_sentence[:100]}...")
    print(f"S√©quence (10 premiers IDs): {test_sequence[:10]}")

    # Conversion inverse pour v√©rification
    reverse_word_index = {v: k for k, v in tokenizer.word_index.items()}
    decoded = [reverse_word_index.get(id, '<UNK>') for id in test_sequence[:10]]
    print(f"Mots d√©cod√©s: {decoded}")

    # Sauvegarde du tokenizer
    tokenizer_file = f"{DATA_PATH}/tokenizer.pkl"
    with open(tokenizer_file, 'wb') as f:
        pickle.dump(tokenizer, f)

    print(f"\nüíæ Tokenizer sauvegard√©: {tokenizer_file}")
    print("‚úÖ CELLULE 4 TERMIN√âE - Passez √† la cellule 5")

# ==============================================================================
# CELLULE 5 : G√âN√âRATION DES DONN√âES SKIP-GRAM
# ==============================================================================

print("üöÄ CELLULE 5: G√©n√©ration des donn√©es Skip-gram")
print("="*50)

if 'tokenizer' not in locals() or 'df_clean' not in locals():
    print("üõë ERREUR - Variables manquantes. Ex√©cutez d'abord les cellules pr√©c√©dentes")
else:

    def create_skipgram_pairs(tokens, window_size=2):
        """Cr√©e les paires (mot_centre, mot_contexte) pour Skip-gram"""
        if len(tokens) <= window_size * 2:
            return [], []

        center_words = []
        context_words = []

        for i in range(window_size, len(tokens) - window_size):
            center_word = tokens[i]

            # Collecte tous les mots contexte dans la fen√™tre
            for j in range(i - window_size, i + window_size + 1):
                if j != i:  # Exclure le mot centre lui-m√™me
                    context_words.append(tokens[j])
                    center_words.append(center_word)

        return center_words, context_words

    # Configuration
    WINDOW_SIZE = 2
    MAX_PAIRS = 500000  # Limite pour √©viter surcharge m√©moire

    print(f"‚öôÔ∏è  Configuration:")
    print(f"   - Taille de fen√™tre: {WINDOW_SIZE}")
    print(f"   - Limite de paires: {MAX_PAIRS:,}")

    # G√©n√©ration des paires d'entra√Ænement
    print(f"\nüîÑ G√©n√©ration des paires sur {len(df_clean):,} reviews...")

    X, y = [], []
    total_pairs = 0
    processed_reviews = 0

    for idx, review in enumerate(tqdm(df_clean.review_clean, desc="G√©n√©ration")):
        # Conversion en s√©quence d'entiers
        sequences = tokenizer.texts_to_sequences([review])

        if len(sequences) > 0 and len(sequences[0]) > WINDOW_SIZE * 2:
            tokens = sequences[0]

            # G√©n√©ration des paires skip-gram
            centers, contexts = create_skipgram_pairs(tokens, WINDOW_SIZE)

            X.extend(centers)
            y.extend(contexts)
            total_pairs += len(centers)
            processed_reviews += 1

        # Arr√™t si limite atteinte
        if total_pairs >= MAX_PAIRS:
            print(f"\n‚ö†Ô∏è  Limite de {MAX_PAIRS:,} paires atteinte")
            break

    # Conversion en arrays numpy
    X = np.array(X, dtype=np.int32)
    y = np.array(y, dtype=np.int32)

    print(f"\n‚úÖ G√©n√©ration termin√©e!")
    print(f"üìä Statistiques:")
    print(f"   - Reviews trait√©es: {processed_reviews:,}")
    print(f"   - Paires g√©n√©r√©es: {len(X):,}")
    print(f"   - Shape X (centres): {X.shape}")
    print(f"   - Shape y (contextes): {y.shape}")
    print(f"   - M√©moire utilis√©e: {(X.nbytes + y.nbytes) / 1024 / 1024:.1f} MB")

    # Affichage d'exemples (CORRIG√â)
    print(f"\nüîç Exemples de paires (mot_centre -> mot_contexte):")
    reverse_word_index = {v: k for k, v in tokenizer.word_index.items()}

    for i in range(min(10, len(X))):
        center_id = X[i]
        context_id = y[i]

        center_word = reverse_word_index.get(center_id, f"UNK_{center_id}")
        context_word = reverse_word_index.get(context_id, f"UNK_{context_id}")

        print(f"   {i+1:2d}. {center_word:<15} -> {context_word}")

    # Statistiques des IDs
    print(f"\nüìà Statistiques des IDs:")
    print(f"   - ID min centres: {X.min()}, max: {X.max()}")
    print(f"   - ID min contextes: {y.min()}, max: {y.max()}")
    print(f"   - Vocabulaire attendu: 1 √† {vocab_size-1}")

    # Sauvegarde des donn√©es d'entra√Ænement
    training_file = f"{DATA_PATH}/training_data.npz"
    np.savez_compressed(
        training_file,
        X=X,
        y=y,
        vocab_size=vocab_size,
        window_size=WINDOW_SIZE,
        total_pairs=len(X)
    )

    print(f"\nüíæ Donn√©es sauvegard√©es: {training_file}")
    file_size = os.path.getsize(training_file) / 1024 / 1024
    print(f"   Taille du fichier: {file_size:.1f} MB")

    print("‚úÖ CELLULE 5 TERMIN√âE - Passez √† la cellule 6")

# ==============================================================================
# CELLULE 6 : CR√âATION DU MOD√àLE WORD2VEC
# ==============================================================================

print("üöÄ CELLULE 6: Cr√©ation du mod√®le Word2Vec")
print("="*50)

if 'X' not in locals() or 'vocab_size' not in locals():
    print("üõë ERREUR - Donn√©es d'entra√Ænement manquantes. Ex√©cutez d'abord la cellule 5")
else:

    # Configuration du mod√®le
    EMBEDDING_DIM = 100  # Dimension des vecteurs word2vec

    print(f"‚öôÔ∏è  Configuration du mod√®le:")
    print(f"   - Taille du vocabulaire: {vocab_size:,}")
    print(f"   - Dimension des embeddings: {EMBEDDING_DIM}")
    print(f"   - Architecture: Skip-gram avec Softmax")

    # Cr√©ation du mod√®le Skip-gram
    def create_skipgram_model(vocab_size, embedding_dim):
        """Cr√©e un mod√®le Word2Vec Skip-gram"""

        model = tf.keras.Sequential([
            # Couche d'embedding - c'est ici que sont stock√©s les vecteurs word2vec
            tf.keras.layers.Embedding(
                input_dim=vocab_size,
                output_dim=embedding_dim,
                input_length=1,
                name='word_embedding'
            ),
            # Aplatit les embeddings
            tf.keras.layers.Flatten(),
            # Couche de sortie pour pr√©dire le mot contexte
            tf.keras.layers.Dense(vocab_size, activation='softmax', name='context_prediction')
        ])

        return model

    # Cr√©ation du mod√®le avec optimisation GPU/CPU
    device = '/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'
    print(f"   - Device utilis√©: {device}")

    with tf.device(device):
        model = create_skipgram_model(vocab_size, EMBEDDING_DIM)

    # Compilation du mod√®le
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    print(f"\nüèóÔ∏è  Architecture du mod√®le:")
    model.summary()

    # Statistiques du mod√®le
    model.build(input_shape=(None, 1))
    total_params = model.count_params()
    embedding_params = vocab_size * EMBEDDING_DIM
    output_params = EMBEDDING_DIM * vocab_size + vocab_size

    print(f"\nüìä Param√®tres du mod√®le:")
    print(f"   - Param√®tres totaux: {total_params:,}")
    print(f"   - Param√®tres embedding: {embedding_params:,}")
    print(f"   - Param√®tres sortie: {output_params:,}")
    print(f"   - Taille estim√©e: {total_params * 4 / 1024 / 1024:.1f} MB")

    # Forme des donn√©es d'entra√Ænement
    print(f"\nüìè V√©rification des donn√©es:")
    print(f"   - X (mots centres): {X.shape}")
    print(f"   - y (mots contextes): {y.shape}")
    print(f"   - Exemple X[0]: {X[0]} -> y[0]: {y[0]}")

    # Test rapide de pr√©diction (avant entra√Ænement)
    print(f"\nüß™ Test de pr√©diction (avant entra√Ænement):")
    test_input = X[:5]
    test_pred = model.predict(test_input, verbose=0)
    print(f"   - Input shape: {test_input.shape}")
    print(f"   - Output shape: {test_pred.shape}")
    print(f"   - Somme des probabilit√©s (doit √™tre ‚âà 1.0): {test_pred[0].sum():.3f}")

    print("‚úÖ CELLULE 6 TERMIN√âE - Mod√®le cr√©√© et pr√™t pour l'entra√Ænement")
    print("‚û°Ô∏è  Passez √† la cellule 7 pour le test rapide")

# ==============================================================================
# CELLULE 7 : TEST RAPIDE DU MOD√àLE
# ==============================================================================

print("üöÄ CELLULE 7: Test rapide du mod√®le")
print("="*50)

if 'model' not in locals() or 'X' not in locals():
    print("üõë ERREUR - Mod√®le ou donn√©es manquants. Ex√©cutez les cellules pr√©c√©dentes")
else:

    # Configuration du test
    TEST_SIZE = 10000  # Nombre d'√©chantillons pour le test
    TEST_EPOCHS = 3    # Nombre d'√©poques pour le test
    BATCH_SIZE = 256

    # Pr√©paration des donn√©es de test
    test_size = min(TEST_SIZE, len(X))
    X_test = X[:test_size]
    y_test = y[:test_size]

    print(f"üß™ Configuration du test:")
    print(f"   - √âchantillons: {test_size:,} / {len(X):,}")
    print(f"   - √âpoques: {TEST_EPOCHS}")
    print(f"   - Batch size: {BATCH_SIZE}")
    print(f"   - Validation split: 10%")

    # Lancement du test
    print(f"\nüöÄ Lancement du test rapide...")
    start_time = time.time()

    # Entra√Ænement de test
    test_history = model.fit(
        X_test, y_test,
        batch_size=BATCH_SIZE,
        epochs=TEST_EPOCHS,
        validation_split=0.1,
        verbose=1,
        shuffle=True
    )

    test_duration = time.time() - start_time

    # R√©sultats du test
    print(f"\nüìä R√©sultats du test:")
    print(f"   ‚è±Ô∏è  Dur√©e: {test_duration:.1f} secondes")
    print(f"   üìà Accuracy finale: {test_history.history['accuracy'][-1]:.4f}")
    print(f"   üìâ Loss finale: {test_history.history['loss'][-1]:.4f}")

    if 'val_accuracy' in test_history.history:
        print(f"   üìà Val Accuracy: {test_history.history['val_accuracy'][-1]:.4f}")
        print(f"   üìâ Val Loss: {test_history.history['val_loss'][-1]:.4f}")

    # Estimations pour l'entra√Ænement complet
    FULL_EPOCHS = 20
    samples_per_second = test_size / test_duration * TEST_EPOCHS
    estimated_full_time = (len(X) * FULL_EPOCHS) / samples_per_second

    print(f"\n‚è∞ Estimations pour l'entra√Ænement complet:")
    print(f"   - Vitesse: {samples_per_second:,.0f} √©chantillons/seconde")
    print(f"   - Pour {FULL_EPOCHS} √©poques sur {len(X):,} √©chantillons:")
    print(f"   - Temps estim√©: {estimated_full_time/60:.1f} minutes")
    print(f"   - Temps estim√©: {estimated_full_time/3600:.2f} heures")

    # Test de similarit√© basique
    print(f"\nüîç Test des embeddings apr√®s {TEST_EPOCHS} √©poques:")

    # Extraction des embeddings
    embeddings = model.get_layer('word_embedding').get_weights()[0]
    print(f"   - Shape des embeddings: {embeddings.shape}")

    # Test sur quelques mots fr√©quents
    reverse_word_index = {v: k for k, v in tokenizer.word_index.items()}
    test_words = []

    # Trouve quelques mots fr√©quents pour le test
    for word_id in range(1, min(100, len(reverse_word_index))):
        if word_id in reverse_word_index:
            word = reverse_word_index[word_id]
            if len(word) > 3:  # Mots de plus de 3 lettres
                test_words.append((word, word_id))
            if len(test_words) >= 5:
                break

    print(f"   - Mots test√©s: {[word for word, _ in test_words]}")

    # Calcul de similarit√©s basiques
    if test_words:
        word1, id1 = test_words[0]
        vec1 = embeddings[id1]

        print(f"   - Vecteur '{word1}' (5 premiers): {vec1[:5]}")
        print(f"   - Norme du vecteur: {np.linalg.norm(vec1):.3f}")

    # Graphique de l'√©volution (si matplotlib disponible)
    try:
        plt.figure(figsize=(12, 4))

        plt.subplot(1, 2, 1)
        plt.plot(test_history.history['loss'], label='Training Loss')
        plt.plot(test_history.history['val_loss'], label='Validation Loss')
        plt.title('√âvolution de la perte')
        plt.xlabel('√âpoque')
        plt.ylabel('Loss')
        plt.legend()

        plt.subplot(1, 2, 2)
        plt.plot(test_history.history['accuracy'], label='Training Accuracy')
        plt.plot(test_history.history['val_accuracy'], label='Validation Accuracy')
        plt.title("√âvolution de l'accuracy")
        plt.xlabel('√âpoque')
        plt.ylabel('Accuracy')
        plt.legend()

        plt.tight_layout()
        plt.show()
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors de l'affichage des graphiques: {e}")

    print("‚úÖ CELLULE 7 TERMIN√âE - Test rapide effectu√©")
    print("‚û°Ô∏è  Vous pouvez passer √† l'entra√Ænement complet ou explorer les embeddings")

# ==============================================================================
# CELLULE 8 : ENTRA√éNEMENT COMPLET DU MOD√àLE WORD2VEC
# ==============================================================================

print("üöÄ CELLULE 8: Entra√Ænement complet du mod√®le Word2Vec")
print("="*50)

if 'model' not in locals() or 'X' not in locals():
    print("üõë ERREUR - Mod√®le ou donn√©es manquants. Ex√©cutez les cellules pr√©c√©dentes")
else:
    # Configuration
    FULL_EPOCHS = 50       # Nombre d'√©poques compl√®tes
    BATCH_SIZE = 128       # Batch size (augmentez si vous avez plus de VRAM)
    VALIDATION_SPLIT = 0.1 # Validation automatique

    print(f"‚öôÔ∏è  Param√®tres d'entra√Ænement:")
    print(f"   - √âpoques: {FULL_EPOCHS}")
    print(f"   - Batch size: {BATCH_SIZE}")
    print(f"   - √âchantillons: {len(X):,}")
    print(f"   - Validation split: {VALIDATION_SPLIT * 100:.0f}%")

    # Entra√Ænement complet
    start_time = time.time()
    full_history = model.fit(
        X, y,
        batch_size=BATCH_SIZE,
        epochs=FULL_EPOCHS,
        validation_split=VALIDATION_SPLIT,
        shuffle=True,
        verbose=1
    )
    duration = time.time() - start_time

    print(f"\n‚úÖ Entra√Ænement termin√© en {duration/60:.1f} minutes")

    # Graphiques d'√©volution
    try:
        plt.figure(figsize=(12, 4))

        plt.subplot(1, 2, 1)
        plt.plot(full_history.history['loss'], label='Training Loss')
        plt.plot(full_history.history['val_loss'], label='Validation Loss')
        plt.title("√âvolution de la perte")
        plt.xlabel("√âpoque")
        plt.ylabel("Loss")
        plt.legend()

        plt.subplot(1, 2, 2)
        plt.plot(full_history.history['accuracy'], label='Training Accuracy')
        plt.plot(full_history.history['val_accuracy'], label='Validation Accuracy')
        plt.title("√âvolution de l'accuracy")
        plt.xlabel("√âpoque")
        plt.ylabel("Accuracy")
        plt.legend()

        plt.tight_layout()
        plt.show()
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors de l'affichage des graphiques: {e}")

    print("üì¶ Le mod√®le est maintenant pr√™t √† √™tre utilis√© pour des tests, visualisations ou exports.")

model.save('/content/drive/MyDrive/Colab Notebooks/DL_Word2Vec/Model/word2vec.h5')